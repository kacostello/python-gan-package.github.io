{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Python GAN Package Here is a documentation tool that describes the Python GAN Package components. Be sure to utilize the different tabs to learn about each component. To learn more general information, please select the About tab, or select the next button.","title":"Home"},{"location":"#welcome-to-python-gan-package","text":"Here is a documentation tool that describes the Python GAN Package components. Be sure to utilize the different tabs to learn about each component. To learn more general information, please select the About tab, or select the next button.","title":"Welcome to Python GAN Package"},{"location":"SimpleGANTrainer.py/","text":"About SimpleGANTrainer.py SimpleGANTrainer is an object designed to train, store, and evaluate simple GANs. That is to say, any GAN with one generator and one discriminator where the generator is directly trained from the output from the discriminator. The Trainer object is designed to be able to train any GAN which meets this description, and automatically keep track of selected metrics during training so as to easily review the training process. This tutorial will show how to create and train a simple GAN using the Trainer object, and will go over some ways to use the Trainer object\u2019s functionality in more advanced ways. Setup The Trainer object requires some setup before it can be created. In particular, it requires: Two pytorch models (the generator and the discriminator) The optimizer and loss function for each model A function to draw from the latent space A function to draw from the real data The device on which to train the models Optionally, the Trainer object can also take: A ToTrain object The desired positive result threshold for the discriminator, used for calculating certain metrics Designing the GAN Before we can train a GAN, we need to know what we want the GAN to do. For this tutorial, we will create an extremely simple GAN - the generator will generate 7-bit even binary numbers, and the discriminator will distinguish between even and odd 7-bit binary numbers. Models The simplest possible generator which meets our requirements is a single layer consisting of 7 inputs and outputs and with a Sigmoid activation. This model is defined as follows:\\ # Generator class Generator(nn.Module): def __init__(self): super(Generator, self).__init__() self.dense_layer = nn.Linear(7, 7) self.activation = nn.Sigmoid() def forward(self, x): return self.activation(self.dense_layer(x)) Our discriminator is similarly simple. It consists of a single layer with 7 inputs and 1 output, again with a Sigmoid activation. It is defined as follows: # Discriminator class Discriminator(nn.Module): def __init__(self): super(Discriminator, self).__init__() self.dense = nn.Linear(7, 1) self.activation = nn.Sigmoid() def forward(self, x): return self.activation(self.dense(x)) Finally, we create the model objects: # Model objects gen = Generator() dis = Discriminator() The Trainer object stores the models in its models dictionary. Specifically, the models dictionary is of the form: {\u201cG\u201d:generator, \u201cD\u201d:discriminator}. The models are kept in training mode normally, though the discriminator is set to eval mode while training the generator, and the eval(model, in_dat) function sets the specified model to eval mode before evaluating, and returns it to train mode afterwards. Optimizers and Loss Functions For our GAN we simply use built-in optimizers and loss functions: # built-in optimizers and loss functions: gen_opt = torch.optim.Adam(gen.parameters(), lr=0.001) dis_opt = torch.optim.Adam(dis.parameters(), lr=0.001) gen_loss = nn.BCELoss() dis_loss = nn.BCELoss() However, any optimizers and loss functions which work in a pytorch training loop, including custom objects, work perfectly fine with the Trainer object. Latent Space and Dataset Functions We now need functions to draw from the latent space and dataset. The latent space is the parameter space from which the generator draws. For our GAN, this is just a simple random tensor of size 7: # random tensor of size 7 def lat_space(batch_size, dev): return torch.randint(0, 2, size=(batch_size, 7), device=dev).float() The dataset function is a function designed to return a batch of real data. Real data for us is just an even number, so it\u2019s easier to generate data than retrieve it from a database. # dataset function def batch_from_data(batch_size, dev): max_int = 128 # Get the number of binary places needed to represent the maximum number max_length = int(math.log(max_int, 2)) # Sample batch_size number of integers in range 0-max_int sampled_integers = np.random.randint(0, int(max_int / 2), batch_size) # create a list of labels all ones because all numbers are even labels = [1] * batch_size # Generate a list of binary numbers for training. data = [list_from_num(int(x * 2)) for x in sampled_integers] data = [([0] * (max_length - len(x))) + x for x in data] return torch.tensor(data, device=dev).float() Both the latent space and dataset functions take the parameters (batch_size, device). The batch_size parameter determines the size of the batch, and the device parameter is the device on which the tensors are created. The functions must output a tensor of the shape (batch_size, input_size) - the outputs of the latent space and dataset functions are passed directly into the generator and discriminator respectively. Device The Trainer object supports training on any device visible to PyTorch. We want to train on the GPU, so we use: # train on GPU GAN_DEVICE = \"cuda\" If we do not have a GPU, we would use: # train without GPU GAN_DEVICE = \"cpu\" ToTrain Object ToTrain objects are objects designed to determine which model to train during the training process. The package comes with a number of built-in ToTrain objects, and they are designed to be as easy as possible to build your own custom ToTrain object. Our GAN just uses the Two-Five Rule ToTrain object, which trains the generator for two epochs then trains the discriminator for five epochs. Discriminator Positive Threshold The Trainer object allows the user to specify the threshold above which output from the discriminator is considered to be positive. This only impacts calculation of certain metrics (precision, recall, and false positive rate specifically), and does not affect training. By default, this parameter is set to 0.5 if not specified. This is fine for our purposes, and so we do not set this parameter. Creating the Trainer Object All that is left to do is to create the trainer object. This is done by: # trainer object creation gan = SimpleGANTrainer(gen, dis, lat_space, batch_from_data, gen_loss, dis_loss, gen_opt, dis_opt, device, sw) Training the GAN With our Trainer object created, we can now train it at will. To train a GAN, call the .train(epochs, batch_size) function: # call to train GAN gan.train(7000, 16) This will train the generator and discriminator according to the ToTrain object we specified. With the Two-Five Rule ToTrain object, this trains the generator for a total of 2,000 epochs and the discriminator for a total of 5,000 epochs. Trainer objects can train for any length of time, across any number of different invocations of the .train() function. The function is blocking, though, so if we want to see output in the middle of training we must call the .train() function multiple times: [TODO: make these into screenshots of code] gan.train(2000, 16) gan.loss_by_epoch(\u201cG\u201d) # Graphs the generator\u2019s loss for the first 2000 epochs gan.train(5000, 16) The state of the ToTrain object is preserved across multiple calls to .train(), so gan.train(2, 16) gan.train(5, 16) is equivalent to gan.train(7, 16) Evaluating the Models The model objects can be directly accessed through the models dictionary. The Trainer object also has the .eval(model, in_dat) function, or the .eval_generator(in_dat) and .eval_discriminator(in_dat) functions (which just call self.eval(\u201cG\u201d, in_dat) and self.eval(\u201cD\u201d, in_dat) respectively). To see output from the trained generator: # output from trained generator print(gan.eval_generator(lat_space(16, device))) Evaluating on a Different Device The Trainer object supports moving the models to different devices, so it\u2019s possible to use Trainer objects to train and evaluate models on different devices. Use the .models_to(new_device) function to send all models to the specified device. To train the models on the GPU and evaluate on the CPU, for instance, we would: # evaluate on different device GAN_DEVICE = \"cuda\" gan = SimpleGANTrainer(gen, dis, lat_space, batch_from_data, gen_loss, dis_loss, gen_opt, dis_opt, device, sw) gan.train(7000, 16) print(gan.eval_generator(lat_space(16, device))) gan.loss_by_epoch_d() Visualizing Training Loss by Epoch Trainer objects save certain metrics of data in order to allow the user to see how the models are performing. These visualizers include: sw = TwoFiveRule() Divergence by Epoch Shows a graph of the Wasserstein distance of the generator per epoch. Called with .divergence_by_epoch() Epochs Trained Returns the total number of epochs which the specified model was trained. Called with .epochs_trained(model)","title":"SimpleGANTrainer"},{"location":"SimpleGANTrainer.py/#about-simplegantrainerpy","text":"SimpleGANTrainer is an object designed to train, store, and evaluate simple GANs. That is to say, any GAN with one generator and one discriminator where the generator is directly trained from the output from the discriminator. The Trainer object is designed to be able to train any GAN which meets this description, and automatically keep track of selected metrics during training so as to easily review the training process. This tutorial will show how to create and train a simple GAN using the Trainer object, and will go over some ways to use the Trainer object\u2019s functionality in more advanced ways.","title":"About SimpleGANTrainer.py"},{"location":"SimpleGANTrainer.py/#setup","text":"The Trainer object requires some setup before it can be created. In particular, it requires: Two pytorch models (the generator and the discriminator) The optimizer and loss function for each model A function to draw from the latent space A function to draw from the real data The device on which to train the models Optionally, the Trainer object can also take: A ToTrain object The desired positive result threshold for the discriminator, used for calculating certain metrics","title":"Setup"},{"location":"SimpleGANTrainer.py/#designing-the-gan","text":"Before we can train a GAN, we need to know what we want the GAN to do. For this tutorial, we will create an extremely simple GAN - the generator will generate 7-bit even binary numbers, and the discriminator will distinguish between even and odd 7-bit binary numbers.","title":"Designing the GAN"},{"location":"SimpleGANTrainer.py/#models","text":"The simplest possible generator which meets our requirements is a single layer consisting of 7 inputs and outputs and with a Sigmoid activation. This model is defined as follows:\\ # Generator class Generator(nn.Module): def __init__(self): super(Generator, self).__init__() self.dense_layer = nn.Linear(7, 7) self.activation = nn.Sigmoid() def forward(self, x): return self.activation(self.dense_layer(x)) Our discriminator is similarly simple. It consists of a single layer with 7 inputs and 1 output, again with a Sigmoid activation. It is defined as follows: # Discriminator class Discriminator(nn.Module): def __init__(self): super(Discriminator, self).__init__() self.dense = nn.Linear(7, 1) self.activation = nn.Sigmoid() def forward(self, x): return self.activation(self.dense(x)) Finally, we create the model objects: # Model objects gen = Generator() dis = Discriminator() The Trainer object stores the models in its models dictionary. Specifically, the models dictionary is of the form: {\u201cG\u201d:generator, \u201cD\u201d:discriminator}. The models are kept in training mode normally, though the discriminator is set to eval mode while training the generator, and the eval(model, in_dat) function sets the specified model to eval mode before evaluating, and returns it to train mode afterwards.","title":"Models"},{"location":"SimpleGANTrainer.py/#optimizers-and-loss-functions","text":"For our GAN we simply use built-in optimizers and loss functions: # built-in optimizers and loss functions: gen_opt = torch.optim.Adam(gen.parameters(), lr=0.001) dis_opt = torch.optim.Adam(dis.parameters(), lr=0.001) gen_loss = nn.BCELoss() dis_loss = nn.BCELoss() However, any optimizers and loss functions which work in a pytorch training loop, including custom objects, work perfectly fine with the Trainer object.","title":"Optimizers and Loss Functions"},{"location":"SimpleGANTrainer.py/#latent-space-and-dataset-functions","text":"We now need functions to draw from the latent space and dataset. The latent space is the parameter space from which the generator draws. For our GAN, this is just a simple random tensor of size 7: # random tensor of size 7 def lat_space(batch_size, dev): return torch.randint(0, 2, size=(batch_size, 7), device=dev).float() The dataset function is a function designed to return a batch of real data. Real data for us is just an even number, so it\u2019s easier to generate data than retrieve it from a database. # dataset function def batch_from_data(batch_size, dev): max_int = 128 # Get the number of binary places needed to represent the maximum number max_length = int(math.log(max_int, 2)) # Sample batch_size number of integers in range 0-max_int sampled_integers = np.random.randint(0, int(max_int / 2), batch_size) # create a list of labels all ones because all numbers are even labels = [1] * batch_size # Generate a list of binary numbers for training. data = [list_from_num(int(x * 2)) for x in sampled_integers] data = [([0] * (max_length - len(x))) + x for x in data] return torch.tensor(data, device=dev).float() Both the latent space and dataset functions take the parameters (batch_size, device). The batch_size parameter determines the size of the batch, and the device parameter is the device on which the tensors are created. The functions must output a tensor of the shape (batch_size, input_size) - the outputs of the latent space and dataset functions are passed directly into the generator and discriminator respectively.","title":"Latent Space and Dataset Functions"},{"location":"SimpleGANTrainer.py/#device","text":"The Trainer object supports training on any device visible to PyTorch. We want to train on the GPU, so we use: # train on GPU GAN_DEVICE = \"cuda\" If we do not have a GPU, we would use: # train without GPU GAN_DEVICE = \"cpu\"","title":"Device"},{"location":"SimpleGANTrainer.py/#totrain-object","text":"ToTrain objects are objects designed to determine which model to train during the training process. The package comes with a number of built-in ToTrain objects, and they are designed to be as easy as possible to build your own custom ToTrain object. Our GAN just uses the Two-Five Rule ToTrain object, which trains the generator for two epochs then trains the discriminator for five epochs.","title":"ToTrain Object"},{"location":"SimpleGANTrainer.py/#discriminator-positive-threshold","text":"The Trainer object allows the user to specify the threshold above which output from the discriminator is considered to be positive. This only impacts calculation of certain metrics (precision, recall, and false positive rate specifically), and does not affect training. By default, this parameter is set to 0.5 if not specified. This is fine for our purposes, and so we do not set this parameter.","title":"Discriminator Positive Threshold"},{"location":"SimpleGANTrainer.py/#creating-the-trainer-object","text":"All that is left to do is to create the trainer object. This is done by: # trainer object creation gan = SimpleGANTrainer(gen, dis, lat_space, batch_from_data, gen_loss, dis_loss, gen_opt, dis_opt, device, sw)","title":"Creating the Trainer Object"},{"location":"SimpleGANTrainer.py/#training-the-gan","text":"With our Trainer object created, we can now train it at will. To train a GAN, call the .train(epochs, batch_size) function: # call to train GAN gan.train(7000, 16) This will train the generator and discriminator according to the ToTrain object we specified. With the Two-Five Rule ToTrain object, this trains the generator for a total of 2,000 epochs and the discriminator for a total of 5,000 epochs. Trainer objects can train for any length of time, across any number of different invocations of the .train() function. The function is blocking, though, so if we want to see output in the middle of training we must call the .train() function multiple times: [TODO: make these into screenshots of code] gan.train(2000, 16) gan.loss_by_epoch(\u201cG\u201d) # Graphs the generator\u2019s loss for the first 2000 epochs gan.train(5000, 16) The state of the ToTrain object is preserved across multiple calls to .train(), so gan.train(2, 16) gan.train(5, 16) is equivalent to gan.train(7, 16)","title":"Training the GAN"},{"location":"SimpleGANTrainer.py/#evaluating-the-models","text":"The model objects can be directly accessed through the models dictionary. The Trainer object also has the .eval(model, in_dat) function, or the .eval_generator(in_dat) and .eval_discriminator(in_dat) functions (which just call self.eval(\u201cG\u201d, in_dat) and self.eval(\u201cD\u201d, in_dat) respectively). To see output from the trained generator: # output from trained generator print(gan.eval_generator(lat_space(16, device)))","title":"Evaluating the Models"},{"location":"SimpleGANTrainer.py/#evaluating-on-a-different-device","text":"The Trainer object supports moving the models to different devices, so it\u2019s possible to use Trainer objects to train and evaluate models on different devices. Use the .models_to(new_device) function to send all models to the specified device. To train the models on the GPU and evaluate on the CPU, for instance, we would: # evaluate on different device GAN_DEVICE = \"cuda\" gan = SimpleGANTrainer(gen, dis, lat_space, batch_from_data, gen_loss, dis_loss, gen_opt, dis_opt, device, sw) gan.train(7000, 16) print(gan.eval_generator(lat_space(16, device))) gan.loss_by_epoch_d()","title":"Evaluating on a Different Device"},{"location":"SimpleGANTrainer.py/#visualizing-training","text":"","title":"Visualizing Training"},{"location":"SimpleGANTrainer.py/#loss-by-epoch","text":"Trainer objects save certain metrics of data in order to allow the user to see how the models are performing. These visualizers include: sw = TwoFiveRule()","title":"Loss by Epoch"},{"location":"SimpleGANTrainer.py/#divergence-by-epoch","text":"Shows a graph of the Wasserstein distance of the generator per epoch. Called with .divergence_by_epoch()","title":"Divergence by Epoch"},{"location":"SimpleGANTrainer.py/#epochs-trained","text":"Returns the total number of epochs which the specified model was trained. Called with .epochs_trained(model)","title":"Epochs Trained"},{"location":"SuperTrainer.py/","text":"About SuperTrainer.py SuperTrainer has these attributes: ToTrain: ToTrain object, which determines which model gets trained on a given epoch. Can be as simple or complex as needed, and can be fed whatever input makes sense for the specific type of GAN. ToTrain objects all have a .next(trainer) function, which takes as an argument the trainer object and returns the string representation of the next model to train. Model names: Not explicitly stored, these are the strings used to refer to each model in each dictionary. in the SimpleGAN implementation, these are \"D\" and \"G\" for the discriminator and generator respectively. Models: Dictionary of the form {model name: pytorch model}. In the SimpleGAN implementation, this is {\"D\":discriminator, \"G\":generator} in_functions: Dictionary of the functions which returns the data to be fed into a model, and the labels for that data. Each function takes in whatever it needs to generate this information, and outputs [model_in, true]. In the SimpleGAN implementation, this is {\"D\":discriminator_input, \"G\":generator_input}. In the SimpleGAN implementation, both in_functions return data to be fed into the discriminator. Example: generator_input takes in batch size, generator model object, and the latent space function. It creates a batch of the latent space, and feeds this batch to the generator. The generator wants to always fool the discriminator, so the labels are all 1. The function then returns [generator_out, labels]. After calling this function, the training loop feeds generator_out to the discriminator, and then feeds (discriminator_out, labels) to the generator's loss function. By default, the label for fake data is 0 and real data is 1. loss_functions: Dictionary of the PyTorch loss functions. These functions take in the (model_output, labels) pair, and returns the PyTorch object used for model training. opts: Dictionary of the PyTorch optimizers for each model. These are the optimizer objects used for training with PyTorch. SuperTrainer's Stats Dictionary: The stats dictionary mapping names of statistics with dictionaries containing those statistics. It is of the format {stat_name:stat_dict} losses: Dictionary of the record of the numerical loss values over every training epoch. After training is complete, it should be of the structure {model_name:[loss_epoch_0, loss_epoch_1, ..., loss_epoch_n]}. Used for the loss_by_epoch visualization. epochs_trained: Dictionary of the number of epochs each model has been trained. After training is complete, it should be of the structure {model_name:epochs_model_trained} # Training loop process for Simple GAN: for each epoch: x = self.totrain.next(self) # x = \"G\" or \"D\", following a 2:5 rule if x == \"G\": dis_in, labels = in_functions[x](batch_size, generator, latent_space_function) # generator_input() else: dis_in, labels = in_functions[x](batch_size, generator, latent_space_function, from_dataset_function) # discriminator_input() predicted = models[\"D\"](dis_in) loss = loss_functions[x](predicted, labels) # g_loss() or d_loss() # add numerical loss to losses[x] opts[x].zero_grad() # g_opt or d_opt.zero_grad() loss.backward() self.opts[x].step()","title":"SuperTrainer"},{"location":"SuperTrainer.py/#about-supertrainerpy","text":"SuperTrainer has these attributes: ToTrain: ToTrain object, which determines which model gets trained on a given epoch. Can be as simple or complex as needed, and can be fed whatever input makes sense for the specific type of GAN. ToTrain objects all have a .next(trainer) function, which takes as an argument the trainer object and returns the string representation of the next model to train. Model names: Not explicitly stored, these are the strings used to refer to each model in each dictionary. in the SimpleGAN implementation, these are \"D\" and \"G\" for the discriminator and generator respectively. Models: Dictionary of the form {model name: pytorch model}. In the SimpleGAN implementation, this is {\"D\":discriminator, \"G\":generator} in_functions: Dictionary of the functions which returns the data to be fed into a model, and the labels for that data. Each function takes in whatever it needs to generate this information, and outputs [model_in, true]. In the SimpleGAN implementation, this is {\"D\":discriminator_input, \"G\":generator_input}. In the SimpleGAN implementation, both in_functions return data to be fed into the discriminator. Example: generator_input takes in batch size, generator model object, and the latent space function. It creates a batch of the latent space, and feeds this batch to the generator. The generator wants to always fool the discriminator, so the labels are all 1. The function then returns [generator_out, labels]. After calling this function, the training loop feeds generator_out to the discriminator, and then feeds (discriminator_out, labels) to the generator's loss function. By default, the label for fake data is 0 and real data is 1. loss_functions: Dictionary of the PyTorch loss functions. These functions take in the (model_output, labels) pair, and returns the PyTorch object used for model training. opts: Dictionary of the PyTorch optimizers for each model. These are the optimizer objects used for training with PyTorch. SuperTrainer's Stats Dictionary: The stats dictionary mapping names of statistics with dictionaries containing those statistics. It is of the format {stat_name:stat_dict} losses: Dictionary of the record of the numerical loss values over every training epoch. After training is complete, it should be of the structure {model_name:[loss_epoch_0, loss_epoch_1, ..., loss_epoch_n]}. Used for the loss_by_epoch visualization. epochs_trained: Dictionary of the number of epochs each model has been trained. After training is complete, it should be of the structure {model_name:epochs_model_trained} # Training loop process for Simple GAN: for each epoch: x = self.totrain.next(self) # x = \"G\" or \"D\", following a 2:5 rule if x == \"G\": dis_in, labels = in_functions[x](batch_size, generator, latent_space_function) # generator_input() else: dis_in, labels = in_functions[x](batch_size, generator, latent_space_function, from_dataset_function) # discriminator_input() predicted = models[\"D\"](dis_in) loss = loss_functions[x](predicted, labels) # g_loss() or d_loss() # add numerical loss to losses[x] opts[x].zero_grad() # g_opt or d_opt.zero_grad() loss.backward() self.opts[x].step()","title":"About SuperTrainer.py"},{"location":"WassersteinGANTrainer.py/","text":"About WassersteinGANTrainer.py WassersteinGANTrainer has these attributes: Import Statements: import SuperTrainer import ToTrain import torch import math import torch.optim as optim SuperTrainer and ToTrain are both different classes in the package, while the torch, math, and optim imports are necessary imports that help to implement different functions and optimization algorithms in PyTorch. class WassersteinGANTrainer(SuperTrainer.SuperTrainer): This is the class definition that creates a class to train the Wasserstein GAN. In this class, the Generator and the Discriminator are torch model objects. The Latent Space function (Latent_space_function) is a function which returns an array of n points from the real dataset. optim.RMSprop(generator.parameters(), g_lr): This function implements the RMS prop algorithm for the Generator. More information can be found here. optim.RMSprop(discriminator.parameters(), d_lr): This function implements the RMS prop algorithm for the Discriminator. More information can be found here. # Both input functions return the tuple (dis_in, labels) # generator_in returns (gen_out, labels) - this data is passed through D and used to train G # discriminator_in returns (dis_in, labels) - this is used to train D directly # For other GAN types: input functions can return whatever makes the most sense for your specific GAN # (so controllable GAN, for instance, might want to return a classification vector as well) def train(self, n_epochs, n_batch): all_dists = [] for epoch in range(n_epochs): tt = self.totrain.next(self) # Determine which model to train - sw will either be \"D\" or \"G\" dis_in, y = self.in_functions[tt](n_batch) if tt == \"G\": # If we're training the generator, we should temporarily put the discriminator in eval mode self.models[\"D\"].eval() mod_pred = self.models[\"D\"](dis_in) self.models[\"D\"].train() mod_loss = self.loss_functions[tt](mod_pred, y) # Logging for visualizers self.stats[\"losses\"][tt].append(mod_loss.item()) self.stats[\"epochs_trained\"][tt] += 1 y_flat = y.cpu().numpy().flatten() # Calculate fPr, recall, precision mod_pred_flat = mod_pred.cpu().detach().numpy().flatten() fP = 0 fN = 0 tP = 0 tN = 0 for i in range(len(y_flat)): if y_flat[i] == 0: if mod_pred_flat[i] > self.d_thresh: fP += 1 else: tN += 1 else: if mod_pred_flat[i] > self.d_thresh: tP += 1 else: fN += 1 if fP + tN > 0: self.stats[\"d_fpr\"].append(fP / (fP + tN)) if tP + fP > 0: self.stats[\"d_precision\"].append(tP / (tP + fP)) if tP + fN > 0: self.stats[\"d_recall\"].append(tP / (tP + fN)) # Pytorch training steps self.optimizers[tt].zero_grad() mod_loss.backward() self.optimizers[tt].step() if tt == \"D\": for p in self.models[\"D\"].parameters(): p.data.clamp_(-0.01, 0.01) w_dists = self.all_Wasserstein_dists(self.eval_generator(self.latent_space(256)), self.dataset(256)) w_dist_mean = torch.mean(w_dists) all_dists.append(w_dist_mean) # print(w_dist_mean) print(len(all_dists)) plt.title('Wasserstein GAN Training Over Time') plt.xlabel('Batches') plt.ylabel('Wasserstein Distance Mean') plt.plot(all_dists) plt.show() The above function is the training loop to train the Wasserstein GAN. The train takes in self, the number of epochs, and the batch size as arguments. Next, there is a for loop that is repeated for each epoch in the total number of epochs. Inside of the loop, the input functions return the corresponding tuple. After returning the corresponding tuple, the function then checks if the Generator or the Discriminator is being trained. If the Generator is being trained, then the Discriminator should be put in eval mode for a temporary amount of time. Otherwise, the training continues and the loss function is generated. The logging for visualizers also takes place. After the logging, there are some PyTorch training steps to occur, inducing having the self optimizers call the zero_grad() function. Then, there is some backward propagation and stepping. If the Discriminator is being trained, some of the data gets clamped. # This function evaluates the Generator. def eval_generator(self, in_dat): return self.eval(\"G\", in_dat) #This function evaluates the Discriminator. def eval_discriminator(self, in_dat): return self.eval(\"D\", in_dat) # This function obtains the loss function for the Generator. def get_g_loss_fn(self): return self.loss_functions[\"G\"] # This function obtains the optimizer for the Generator. def get_g_opt_fn(self): return self.optimizers[\"G\"] # This function obtains the loss function for the Discriminator. def get_d_loss_fn(self): return self.loss_functions[\"D\"] # This function obtains the optimizer for the Discriminator. def get_d_opt_fn(self): return self.optimizers[\"D\"] # This function calculates the loss by each epoch for the Generator. def loss_by_epoch_g(self): self.loss_by_epoch(\"G\") # This function calculates the loss by each epoch for the Discriminator. def loss_by_epoch_d(self): self.loss_by_epoch(\"D\") # This function returns the input from the Discriminator. def discriminator_input(self, n_batch): gen_in = self.latent_space(math.ceil(n_batch / 2)) self.models[\"G\"].eval() gen_out = self.models[\"G\"](gen_in) self.models[\"G\"].train() dis_in = torch.cat((gen_out, self.dataset(int(n_batch / 2)))) y = torch.tensor([[0] for n in range(math.ceil(n_batch / 2))] + [[1] for n in range(int(n_batch / 2))]).float() return dis_in, y # This function returns the input from the Generator. def generator_input(self, n_batch): gen_in = self.latent_space(n_batch) gen_out = self.models[\"G\"](gen_in) y = torch.tensor([[1] for n in range(n_batch)]).float() return gen_out, y","title":"WassersteinGANTrainer"},{"location":"WassersteinGANTrainer.py/#about-wassersteingantrainerpy","text":"WassersteinGANTrainer has these attributes: Import Statements: import SuperTrainer import ToTrain import torch import math import torch.optim as optim SuperTrainer and ToTrain are both different classes in the package, while the torch, math, and optim imports are necessary imports that help to implement different functions and optimization algorithms in PyTorch. class WassersteinGANTrainer(SuperTrainer.SuperTrainer): This is the class definition that creates a class to train the Wasserstein GAN. In this class, the Generator and the Discriminator are torch model objects. The Latent Space function (Latent_space_function) is a function which returns an array of n points from the real dataset. optim.RMSprop(generator.parameters(), g_lr): This function implements the RMS prop algorithm for the Generator. More information can be found here. optim.RMSprop(discriminator.parameters(), d_lr): This function implements the RMS prop algorithm for the Discriminator. More information can be found here. # Both input functions return the tuple (dis_in, labels) # generator_in returns (gen_out, labels) - this data is passed through D and used to train G # discriminator_in returns (dis_in, labels) - this is used to train D directly # For other GAN types: input functions can return whatever makes the most sense for your specific GAN # (so controllable GAN, for instance, might want to return a classification vector as well) def train(self, n_epochs, n_batch): all_dists = [] for epoch in range(n_epochs): tt = self.totrain.next(self) # Determine which model to train - sw will either be \"D\" or \"G\" dis_in, y = self.in_functions[tt](n_batch) if tt == \"G\": # If we're training the generator, we should temporarily put the discriminator in eval mode self.models[\"D\"].eval() mod_pred = self.models[\"D\"](dis_in) self.models[\"D\"].train() mod_loss = self.loss_functions[tt](mod_pred, y) # Logging for visualizers self.stats[\"losses\"][tt].append(mod_loss.item()) self.stats[\"epochs_trained\"][tt] += 1 y_flat = y.cpu().numpy().flatten() # Calculate fPr, recall, precision mod_pred_flat = mod_pred.cpu().detach().numpy().flatten() fP = 0 fN = 0 tP = 0 tN = 0 for i in range(len(y_flat)): if y_flat[i] == 0: if mod_pred_flat[i] > self.d_thresh: fP += 1 else: tN += 1 else: if mod_pred_flat[i] > self.d_thresh: tP += 1 else: fN += 1 if fP + tN > 0: self.stats[\"d_fpr\"].append(fP / (fP + tN)) if tP + fP > 0: self.stats[\"d_precision\"].append(tP / (tP + fP)) if tP + fN > 0: self.stats[\"d_recall\"].append(tP / (tP + fN)) # Pytorch training steps self.optimizers[tt].zero_grad() mod_loss.backward() self.optimizers[tt].step() if tt == \"D\": for p in self.models[\"D\"].parameters(): p.data.clamp_(-0.01, 0.01) w_dists = self.all_Wasserstein_dists(self.eval_generator(self.latent_space(256)), self.dataset(256)) w_dist_mean = torch.mean(w_dists) all_dists.append(w_dist_mean) # print(w_dist_mean) print(len(all_dists)) plt.title('Wasserstein GAN Training Over Time') plt.xlabel('Batches') plt.ylabel('Wasserstein Distance Mean') plt.plot(all_dists) plt.show() The above function is the training loop to train the Wasserstein GAN. The train takes in self, the number of epochs, and the batch size as arguments. Next, there is a for loop that is repeated for each epoch in the total number of epochs. Inside of the loop, the input functions return the corresponding tuple. After returning the corresponding tuple, the function then checks if the Generator or the Discriminator is being trained. If the Generator is being trained, then the Discriminator should be put in eval mode for a temporary amount of time. Otherwise, the training continues and the loss function is generated. The logging for visualizers also takes place. After the logging, there are some PyTorch training steps to occur, inducing having the self optimizers call the zero_grad() function. Then, there is some backward propagation and stepping. If the Discriminator is being trained, some of the data gets clamped. # This function evaluates the Generator. def eval_generator(self, in_dat): return self.eval(\"G\", in_dat) #This function evaluates the Discriminator. def eval_discriminator(self, in_dat): return self.eval(\"D\", in_dat) # This function obtains the loss function for the Generator. def get_g_loss_fn(self): return self.loss_functions[\"G\"] # This function obtains the optimizer for the Generator. def get_g_opt_fn(self): return self.optimizers[\"G\"] # This function obtains the loss function for the Discriminator. def get_d_loss_fn(self): return self.loss_functions[\"D\"] # This function obtains the optimizer for the Discriminator. def get_d_opt_fn(self): return self.optimizers[\"D\"] # This function calculates the loss by each epoch for the Generator. def loss_by_epoch_g(self): self.loss_by_epoch(\"G\") # This function calculates the loss by each epoch for the Discriminator. def loss_by_epoch_d(self): self.loss_by_epoch(\"D\") # This function returns the input from the Discriminator. def discriminator_input(self, n_batch): gen_in = self.latent_space(math.ceil(n_batch / 2)) self.models[\"G\"].eval() gen_out = self.models[\"G\"](gen_in) self.models[\"G\"].train() dis_in = torch.cat((gen_out, self.dataset(int(n_batch / 2)))) y = torch.tensor([[0] for n in range(math.ceil(n_batch / 2))] + [[1] for n in range(int(n_batch / 2))]).float() return dis_in, y # This function returns the input from the Generator. def generator_input(self, n_batch): gen_in = self.latent_space(n_batch) gen_out = self.models[\"G\"](gen_in) y = torch.tensor([[1] for n in range(n_batch)]).float() return gen_out, y","title":"About WassersteinGANTrainer.py"},{"location":"about/","text":"PyTorch GAN Package General Information This PyTorch GAN Package includes different GANS using PyTorch and Python programming. The purpose of this package is to give the developer community a useful that will help make and test specific kinds of GANs. The GANs in the package include: Simple GAN Wasserstein GAN Conditional GAN Controllable GAN This package also includes a Super Trainer file, a ToTrain file, and visualizations. The package is designed so the user does not have to write a GAN entirely from scratch. If the user has no expreience with GANs, some experience with Python and opaque Machine Learning, or a GAN expert, this tool will be beneficial. The package design was built with three key features in mind: model building, model training, and model evaluation. For GAN structures that involve novel additions to previous designs, or require the manipulation of technical, subtle changes to the way training is conducted, the package can function as a tool kit for developers and researchers alike.","title":"About"},{"location":"about/#pytorch-gan-package-general-information","text":"This PyTorch GAN Package includes different GANS using PyTorch and Python programming. The purpose of this package is to give the developer community a useful that will help make and test specific kinds of GANs. The GANs in the package include: Simple GAN Wasserstein GAN Conditional GAN Controllable GAN This package also includes a Super Trainer file, a ToTrain file, and visualizations. The package is designed so the user does not have to write a GAN entirely from scratch. If the user has no expreience with GANs, some experience with Python and opaque Machine Learning, or a GAN expert, this tool will be beneficial. The package design was built with three key features in mind: model building, model training, and model evaluation. For GAN structures that involve novel additions to previous designs, or require the manipulation of technical, subtle changes to the way training is conducted, the package can function as a tool kit for developers and researchers alike.","title":"PyTorch GAN Package General Information"},{"location":"data_input/","text":"Data Input Accept numerical/tabular data: numpy matrix (2d) (rows = observations, columns = attributes) If labeled with a class (0,n), another numpy array (1d) (integer index of the class, ie: 1, 2, 0, 3, ....). The size of this array must equal the number of rows in the matrix above Accept image data: single channel (grayscale) numpy tensor (3d) (pixels in the x-axis of image, pixels in the y-axis of image, index of images) If labeled with a class (0, n), another numpy array (1d) (integer index of the class, ie: 1, 2, 0, 3, ....). The size of this array must equal the size of the 3rd dimension in the tensor above.","title":"Data Input"},{"location":"data_input/#data-input","text":"Accept numerical/tabular data: numpy matrix (2d) (rows = observations, columns = attributes) If labeled with a class (0,n), another numpy array (1d) (integer index of the class, ie: 1, 2, 0, 3, ....). The size of this array must equal the number of rows in the matrix above Accept image data: single channel (grayscale) numpy tensor (3d) (pixels in the x-axis of image, pixels in the y-axis of image, index of images) If labeled with a class (0, n), another numpy array (1d) (integer index of the class, ie: 1, 2, 0, 3, ....). The size of this array must equal the size of the 3rd dimension in the tensor above.","title":"Data Input"},{"location":"features/","text":"Key User Features Template can be read as: foo( (required or not){type}) Declare(type{string}, gen(optional){pytorch_architecture}, disc(optional){pytorch_architecture}) - Declare an GAN we want to use, and have practitioner-specified architectures if desired. Train(dataset{numpy_array}, optimizer(optional){pytoroch_optim_object}, drawfromlatentspace(optional){function}, switch_condition(optional){function}, epochs(optional){integer}, lr(optional)={float}) Load(type{string}, gen{pytorch_architecture loaded with trained weights}) Assumes gen is a pretrianed pytorch generator, potentially not trained by this package Save(): saves weight file and the architecture file Sample(n{integer}, class(optional, only applicable for class-generation){integer}) Evaluate(flags_for_each_visualization(optional, multiple arguements){boolean}, to_storage(optional){boolean})","title":"Key User Features"},{"location":"features/#key-user-features","text":"Template can be read as: foo( (required or not){type}) Declare(type{string}, gen(optional){pytorch_architecture}, disc(optional){pytorch_architecture}) - Declare an GAN we want to use, and have practitioner-specified architectures if desired. Train(dataset{numpy_array}, optimizer(optional){pytoroch_optim_object}, drawfromlatentspace(optional){function}, switch_condition(optional){function}, epochs(optional){integer}, lr(optional)={float}) Load(type{string}, gen{pytorch_architecture loaded with trained weights}) Assumes gen is a pretrianed pytorch generator, potentially not trained by this package Save(): saves weight file and the architecture file Sample(n{integer}, class(optional, only applicable for class-generation){integer}) Evaluate(flags_for_each_visualization(optional, multiple arguements){boolean}, to_storage(optional){boolean})","title":"Key User Features"}]}